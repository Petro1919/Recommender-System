{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLEASE NOTE THAT IF YOU WANT TO SEE MY FINAL PREDICTIONS TABLE, YOU NEED TO UNCOMMENT THE PRINT(STORE) IN THE FUNCTION\n",
    "N.B: All questions are explained in more details in the 1 page pdf!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data from the excel file\n",
    "df=pd.read_excel('exerciseCB.xlsx',sheet_name='CB - Simply Unary',header=1,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 3 different dataframes from the 3 different tables in the excel file\n",
    "q_data = df.iloc[:20,:10]\n",
    "user_data=df.iloc[:20,13:17].fillna(0)\n",
    "answer_data=df.iloc[:20,18:22].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simply Unary\n",
    "# Dot product of the user feedback with each topic contained in the questions to which the user gave feedback\n",
    "# Each user ends up with a numeric value for each topic --> user profile\n",
    "# Then I multiply the user profile with the topics contained in each question using the cosine similarity \n",
    "# I then returned the top 5 questions predicted for each user\n",
    "def SimplyUnary(qdata, userdata):\n",
    "    #initializing and defining dataframes for later use\n",
    "    store=pd.DataFrame()\n",
    "    check=qdata.copy()\n",
    "    check_2=qdata.copy()\n",
    "    j=1\n",
    "    #iterating over every topic in the questions for every User that we have using 2 for loops\n",
    "    for i in userdata.columns:\n",
    "        for col in qdata.columns:\n",
    "            #for the non value columns, I multiplied each topic column with the user feedback and summed the result\n",
    "            check_2[col]=np.where(qdata.loc[:,col]==0,qdata.loc[:,col],userdata.loc[:,i]*qdata.loc[:,col])\n",
    "            user_profile=pd.DataFrame(check_2.sum(axis=0)).transpose()\n",
    "            #then I multiplied the obtained sum (as a vector) with the topic each question has, for every user\n",
    "            #using the cosine similarity\n",
    "            mult=cosine_similarity(check,user_profile).sum(axis=1)\n",
    "        store[j]=mult\n",
    "        #clearing the values used for the previous user in order to use them for the next one\n",
    "        user_profile=[]\n",
    "        mult=[]\n",
    "        j=j+1\n",
    "    store.index=userdata.index\n",
    "    # sorting the values contained for every user, and generating the 'top5' questions for every user\n",
    "    top = pd.DataFrame()\n",
    "    for p in store.columns:\n",
    "        #if the user is new we cannot generate recommendations using this method and hence we return this msg\n",
    "        if ((store.loc[:,p].mean())==0 and (store.loc[:,p].std())==0):\n",
    "            top[p]='No Data Available'\n",
    "        else:\n",
    "            rank = store.loc[:,p].sort_values(ascending = False)\n",
    "            top[p]=rank.index\n",
    "    top5=top.loc[:4,:]\n",
    "    top5.columns=userdata.columns\n",
    "    store.columns=userdata.columns\n",
    "    print(store)\n",
    "    return top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  User 1        User 2        User 3  User 4\n",
      "question1   3.903600e-01 -2.981424e-01 -2.927700e-01     0.0\n",
      "question2  -4.364358e-01  8.333333e-01  0.000000e+00     0.0\n",
      "question3   2.519763e-01  1.993498e-17 -3.779645e-01     0.0\n",
      "question4  -3.273268e-01  6.666667e-01 -2.182179e-01     0.0\n",
      "question5  -1.259882e-01  9.622504e-02  2.519763e-01     0.0\n",
      "question6   4.629100e-01  1.178511e-01 -3.086067e-01     0.0\n",
      "question7  -1.543033e-01  2.357023e-01 -1.543033e-01     0.0\n",
      "question8  -2.182179e-01  3.333333e-01  1.091089e-01     0.0\n",
      "question9   4.629100e-01 -2.357023e-01 -4.629100e-01     0.0\n",
      "question10 -3.779645e-01  9.622504e-02 -1.333738e-17     0.0\n",
      "question11  1.333738e-17  9.622504e-02  1.259882e-01     0.0\n",
      "question12  5.039526e-01 -3.849002e-01 -7.559289e-01     0.0\n",
      "question13 -2.182179e-01  5.833333e-01 -1.091089e-01     0.0\n",
      "question14 -2.182179e-01  5.833333e-01  2.182179e-01     0.0\n",
      "question15  0.000000e+00  3.333333e-01 -6.546537e-01     0.0\n",
      "question16  7.559289e-01 -3.849002e-01 -6.299408e-01     0.0\n",
      "question17 -4.364358e-01  8.333333e-01  0.000000e+00     0.0\n",
      "question18  1.543033e-01  3.535534e-01  0.000000e+00     0.0\n",
      "question19 -3.903600e-01  1.490712e-01  1.951800e-01     0.0\n",
      "question20 -1.091089e-01  4.166667e-01  0.000000e+00     0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User 1</th>\n",
       "      <th>User 2</th>\n",
       "      <th>User 3</th>\n",
       "      <th>User 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>question16</td>\n",
       "      <td>question17</td>\n",
       "      <td>question5</td>\n",
       "      <td>No Data Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>question12</td>\n",
       "      <td>question2</td>\n",
       "      <td>question14</td>\n",
       "      <td>No Data Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>question6</td>\n",
       "      <td>question4</td>\n",
       "      <td>question19</td>\n",
       "      <td>No Data Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>question9</td>\n",
       "      <td>question14</td>\n",
       "      <td>question11</td>\n",
       "      <td>No Data Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>question1</td>\n",
       "      <td>question13</td>\n",
       "      <td>question8</td>\n",
       "      <td>No Data Available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User 1      User 2      User 3             User 4\n",
       "0  question16  question17   question5  No Data Available\n",
       "1  question12   question2  question14  No Data Available\n",
       "2   question6   question4  question19  No Data Available\n",
       "3   question9  question14  question11  No Data Available\n",
       "4   question1  question13   question8  No Data Available"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimplyUnary(q_data, user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit Weight\n",
    "#in this function, we will repeat the Simply Unary calculations BUT ALSO assigned a weight to each question \n",
    "#this weight is calculated by taking into account the number of topics a question has\n",
    "#by dividing the topic appearance by the total number of topics a question has\n",
    "# I then returned the top 5 questions predicted for each user\n",
    "def UnitWeight(qdata, userdata):  \n",
    "    # calculating the weight of each question, by dividing with the sum of the topic occurence for every question\n",
    "    test_2=[]\n",
    "    for i in qdata.index:\n",
    "        test_1=1/sum(qdata.loc[i,:])\n",
    "        test_2.append(test_1)\n",
    "    #initializing and defining dataframes for later use\n",
    "    check=qdata.copy()\n",
    "    check_2=qdata.copy()\n",
    "    j=1\n",
    "    store=pd.DataFrame()\n",
    "    #iterating over every topic in the questions for every User that we have using 2 for loops\n",
    "    for i in userdata.columns:\n",
    "        for col in qdata.columns:\n",
    "            #for the non value columns, I multiplied each topic column with the user feedback and I also multiplied\n",
    "            #by the weight that I previously calculated for each question and summed the result\n",
    "            check_2[col]=np.where(qdata.loc[:,col]==0,qdata.loc[:,col],userdata.loc[:,i]*qdata.loc[:,col]*test_2)\n",
    "            user_profile=pd.DataFrame(check_2.sum(axis=0)).transpose()\n",
    "            #then I multiplied the obtained sum (as a vector) with the topic each question has, for every user\n",
    "            #using the cosine similarity\n",
    "            mult=cosine_similarity(check, user_profile).sum(axis=1)\n",
    "        store[j]=mult\n",
    "        #clearing the values used for the previous user in order to use them for the next one\n",
    "        user_profile=[]\n",
    "        standardized_X=[]\n",
    "        mult=[]\n",
    "        j=j+1\n",
    "    # sorting the values contained for every user, and generating the 'top5' questions for every user\n",
    "    store.index=userdata.index    \n",
    "    top = pd.DataFrame()\n",
    "    for p in store.columns:\n",
    "        #if the user is new we cannot generate recommendations using this method and hence we return this msg\n",
    "        if ((store.loc[:,p].mean())==0 and (store.loc[:,p].std())==0):\n",
    "            top[p]='No Data Available'\n",
    "        else:\n",
    "            rank = store.loc[:,p].sort_values(ascending = False)\n",
    "            top[p]=rank.index\n",
    "    top5=top.loc[:4,:]\n",
    "    top5.columns=userdata.columns\n",
    "    store.columns=userdata.columns\n",
    "    #print(store)\n",
    "    return top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnitWeight(q_data, user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF\n",
    "# In addtition to the steps taken in the Unit Weight, we now take the relevance of each topic into account\n",
    "#the higher the number of questions asked for each topic, the lower its relevance is\n",
    "# I then returned the top 5 questions predicted for each user\n",
    "def IDF(qdata, userdata):\n",
    "    #initializing and defining dataframes for later use\n",
    "    test_3=[]\n",
    "    test_2=[]\n",
    "    check=qdata.copy()\n",
    "    # calculating the weight of each question, by dividing with the sum of the topic occurence for every question\n",
    "    for i in qdata.index:\n",
    "        test_1=1/sum(qdata.loc[i,:])\n",
    "        test_2.append(test_1)\n",
    "    #summing the occurence of each topic and then computing the LOG for 20 over that sum in order to have IDF\n",
    "    for i in qdata.columns:\n",
    "        test_1=np.log10(20/sum(qdata.loc[:,i]))\n",
    "        test_3.append(test_1)\n",
    "    test_3=pd.DataFrame(test_3).transpose()\n",
    "    test_3.columns=qdata.columns\n",
    "    check_2=qdata.copy()\n",
    "    j=1\n",
    "    store=pd.DataFrame()\n",
    "    #iterating over every topic in the questions for every User that we have using 2 for loops\n",
    "    for i in userdata.columns:\n",
    "        for col in qdata.columns:\n",
    "            #for the non value columns, I multiplied each topic column with the user feedback and I also multiplied\n",
    "            #by the weight that I previously calculated for each question and by IDF before summing the result\n",
    "            check_2[col]=np.where(qdata.loc[:,col]==0,qdata.loc[:,col],userdata.loc[:,i]*qdata.loc[:,col]*test_2*test_3.loc[0,col])\n",
    "            user_profile=pd.DataFrame(check_2.sum(axis=0)).transpose()\n",
    "            #then I multiplied the obtained sum (as a vector) with the topic each question has, for every user\n",
    "            #using the cosine similarity\n",
    "            mult=cosine_similarity(check, user_profile).sum(axis=1)\n",
    "        store[j]=mult\n",
    "        user_profile=[]\n",
    "        mult=[]\n",
    "        j=j+1\n",
    "    store.index=userdata.index \n",
    "    # sorting the values contained for every user, and generating the 'top5' questions for every user\n",
    "    top = pd.DataFrame()\n",
    "    for p in store.columns:\n",
    "        rank = store.loc[:,p].sort_values(ascending = False)\n",
    "        top[p]=rank.index\n",
    "    top5=top.loc[:4,:]\n",
    "    top5.columns=userdata.columns\n",
    "    store.columns=userdata.columns\n",
    "    #print(store)\n",
    "    return top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF(q_data, user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#SwitchedHybrid\n",
    "# I addition to the steps taken in IDF, we took into account the case of new users who did not give feedback on \n",
    "# any question. We computed their recommendations by taking the average of the other user predictions\n",
    "# I then returned the top 5 questions predicted for each user\n",
    "def SwitchedHybrid(qdata, userdata):  \n",
    "    #initializing and defining lists and dataframes for later use\n",
    "    test_3=[]\n",
    "    test_2=[]\n",
    "    check=qdata.copy()\n",
    "    # calculating the weight of each question, by dividing with the sum of the topic occurence for every question\n",
    "    for i in qdata.index:\n",
    "        test_1=1/sum(qdata.loc[i,:])\n",
    "        test_2.append(test_1)\n",
    "    #summing the occurence of each topic and then computing the LOG for 20 over that sum in order to have IDF\n",
    "    for i in qdata.columns:\n",
    "        test_1=np.log10(20/sum(qdata.loc[:,i]))\n",
    "        test_3.append(test_1)\n",
    "    test_3=pd.DataFrame(test_3).transpose()\n",
    "    test_3.columns=qdata.columns\n",
    "    test_3\n",
    "    j=1\n",
    "    store=pd.DataFrame()\n",
    "    check_2=qdata.copy()\n",
    "    #iterating over every topic in the questions for every User that we have using 2 for loops\n",
    "    for i in userdata.columns:\n",
    "        #if the user is a new one, we will return the mean of the predictions of other users!\n",
    "        if ((userdata.loc[:,i].mean())==0 and (userdata.loc[:,i].std())==0):\n",
    "            store.loc[:,j]=store.loc[:, store.columns != j].mean(axis=1)\n",
    "        else:\n",
    "            for col in qdata.columns:\n",
    "                #for the non value columns, I multiplied each topic column with the user feedback and I also multiplied\n",
    "                #by the weight that I previously calculated for each question and by IDF before summing the result\n",
    "                check_2[col]=np.where(qdata.loc[:,col]==0,qdata.loc[:,col],userdata.loc[:,i]*qdata.loc[:,col]*test_2*test_3.loc[0,col])\n",
    "                user_profile=pd.DataFrame(check_2.sum(axis=0)).transpose()\n",
    "                #then I multiplied the obtained sum (as a vector) with the topic each question has, for every user\n",
    "                #using the cosine similarity\n",
    "                mult=cosine_similarity(check, user_profile).sum(axis=1)\n",
    "            store[j]=mult\n",
    "            user_profile=[]\n",
    "            mult=[]\n",
    "            j=j+1\n",
    "    store.index=userdata.index  \n",
    "    # sorting the values contained for every user, and generating the 'top5' questions for every user\n",
    "    top = pd.DataFrame()\n",
    "    for p in store.columns:\n",
    "        rank = store.loc[:,p].sort_values(ascending = False)\n",
    "        top[p]=rank.index\n",
    "    top5=top.loc[:4,:]\n",
    "    top5.columns=userdata.columns\n",
    "    store.columns=userdata.columns\n",
    "    #print(store)\n",
    "    return top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SwitchedHybrid(q_data, user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hybrid Challenge\n",
    "# I decided to create a recommender that takes into account the IDF methods, the new users and also the VOTES GIVEN\n",
    "# to the answers of the users! Because all previous 4 methods ignored it!\n",
    "# FOR MORE DETAILS PLEASE CHECK THE 1 PAGE SUBMISSION!\n",
    "# I then returned the top 5 questions predicted for each user\n",
    "def HybridChallenge(qdata, userdata, answerdata): \n",
    "    #initializing and defining lists and dataframes for later use\n",
    "    check=qdata.copy()\n",
    "    userdata=userdata.copy()\n",
    "    answerdata.columns=userdata.columns\n",
    "    #we scaled the answers_data in order to have a range of values that match the range of the user feedback\n",
    "    X_2 = preprocessing.scale(answerdata)\n",
    "    X_3 = answerdata.copy()\n",
    "    #since while scaling, all the zeros are transformed to non zero values\n",
    "    # I created a new table, in order to keep the zeros and replace all non zero values with 1\n",
    "    for na in X_3.columns:\n",
    "        for pe in X_3.index:\n",
    "            if (X_3.loc[pe,na]!=0):\n",
    "                X_3.loc[pe,na]=1\n",
    "    #then I multiplied the scaled data(X_2) with the 0 and 1 table in order to get the zeros back \n",
    "    X_2=pd.DataFrame(X_2)\n",
    "    X_4=np.multiply(X_2,X_3)\n",
    "    X_4.columns=answerdata.columns\n",
    "    X_4.index=answerdata.index\n",
    "    answerdata=X_4.copy()\n",
    "    # I took into account the questions that a user didn't give feedback to, but gave a good or bad answer instead\n",
    "    # This is based on the fact that if a user gave a good answer to a question, then we as a PLATFORM want to\n",
    "    # recommend him more of that question, and if he gave a bad feedback then we don't want these questions\n",
    "    for n in userdata.columns:\n",
    "        for p in userdata.index:\n",
    "            if (userdata.loc[p,n]==1):\n",
    "                if (answerdata.loc[p,n]!=0):\n",
    "                    userdata.loc[p,n]=userdata.loc[p,n]*answerdata.loc[p,n]\n",
    "            elif(userdata.loc[p,n]== -1):\n",
    "                if (answerdata.loc[p,n]<0):\n",
    "                    userdata.loc[p,n]=(-1)*userdata.loc[p,n]*answerdata.loc[p,n]\n",
    "                if(answerdata.loc[p,n]>0):\n",
    "                    userdata.loc[p,n]=(-1)*userdata.loc[p,n]*answerdata.loc[p,n]\n",
    "            elif(userdata.loc[p,n]== 0):\n",
    "                if (answerdata.loc[p,n]!=0):\n",
    "                    userdata.loc[p,n]=answerdata.loc[p,n]  \n",
    "    #initializing and defining lists and dataframes for later use\n",
    "    test_3=[]\n",
    "    test_2=[]\n",
    "    # calculating the weight of each question, by dividing with the sum of the topic occurence for every question\n",
    "    for i in qdata.index:\n",
    "        test_1=1/sum(qdata.loc[i,:])\n",
    "        test_2.append(test_1)\n",
    "    #summing the occurence of each topic and then computing the LOG for 20 over that sum in order to have IDF\n",
    "    for i in qdata.columns:\n",
    "        test_1=np.log10(20/sum(qdata.loc[:,i]))\n",
    "        test_3.append(test_1)\n",
    "    test_3=pd.DataFrame(test_3).transpose()\n",
    "    test_3.columns=qdata.columns\n",
    "    test_3\n",
    "    j=1\n",
    "    store=pd.DataFrame()\n",
    "    #iterating over every topic in the questions for every User that we have using 2 for loops\n",
    "    for i in userdata.columns:\n",
    "        #if the user is a new one, we will return the mean of the predictions of other users!\n",
    "        if ((userdata.loc[:,i].mean())==0 and (userdata.loc[:,i].std())==0):\n",
    "            store.loc[:,j]=store.loc[:, store.columns != j].mean(axis=1)\n",
    "        else:\n",
    "            check_2=qdata.copy()\n",
    "            for col in qdata.columns:\n",
    "                #for the non value columns, I multiplied each topic column with the user feedback and I also multiplied\n",
    "                #by the weight that I previously calculated for each question and by IDF before summing the result\n",
    "                check_2[col]=np.where(qdata.loc[:,col]==0,qdata.loc[:,col],userdata.loc[:,i]*qdata.loc[:,col]*test_2*test_3.loc[0,col])\n",
    "                user_profile=pd.DataFrame(check_2.sum(axis=0)).transpose()\n",
    "                #then I multiplied the obtained sum (as a vector) with the topic each question has, for every user\n",
    "                #using the cosine similarity\n",
    "                mult=cosine_similarity(check, user_profile).sum(axis=1)\n",
    "            store[j]=mult\n",
    "            user_profile=[]\n",
    "            mult=[]\n",
    "            j=j+1\n",
    "    store.index=userdata.index \n",
    "    # sorting the values contained for every user, and generating the 'top5' questions for every user\n",
    "    top = pd.DataFrame()\n",
    "    for p in store.columns:\n",
    "        rank = store.loc[:,p].sort_values(ascending = False)\n",
    "        top[p]=rank.index\n",
    "    top5=top.loc[:4,:]\n",
    "    top5.columns=userdata.columns\n",
    "    store.columns=userdata.columns\n",
    "    #print(store)\n",
    "    return top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HybridChallenge(q_data, user_data, answer_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
