{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLEASE COMMENT OUT THE CELLS IN WHICH I AM CALLING THE 100K DATASET IF YOU WISH TO SEE HOW THEY PERFORM!\n",
    "BUT I COMMENTED HOW MUCH TIME THEY TOOK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the movieratings dataset with index col=0 so I can better work with it\n",
    "movieratings=pd.read_csv('movieratings.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please comment out to read the 100K dataset\n",
    "# # reading the 2 datasets by setting the column names\n",
    "# rcolumns = ['User', 'movie_id', 'rating', 'unix_timestamp']\n",
    "# ratings = pd.read_csv('u.data', sep='\\t', names=rcolumns)\n",
    "# mcolumns = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\n",
    "# movies = pd.read_csv('u.item', sep='|', names=mcolumns, usecols=range(5),encoding='latin-1')\n",
    "# #merging movies and ratings datasets\n",
    "# movie_ratings = pd.merge(movies, ratings)\n",
    "# #only taking into account the columns that we need for our recom\n",
    "# movie_ratings=movie_ratings.loc[:,['title','User','rating']]\n",
    "# #transforming the shape of our dataframe so we can have the same shape as the \"movieratings\" one!\n",
    "# hundredKmovie=movie_ratings.reset_index().groupby(['User', 'title'])['rating'].aggregate('first').unstack()\n",
    "# #as opposed to smaller dataset we don't have names here, hence we transform the index to strings to have names of people rather than numbers even if it's only ids\n",
    "# hundredKmovie.index=hundredKmovie.index.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the similarity between 2 users, the similarity is computed from -1 to 1\n",
    "# the function will return a zero in case of no Similarity between the 2 users!\n",
    "def pearsonSimilarity(prefs,person1,person2):\n",
    "    #just in case the 'User' is not in index, I added it!\n",
    "    if 'User' in prefs.columns:\n",
    "        prefs['User']=prefs['User'].astype(str)\n",
    "        prefs=prefs.set_index('User')\n",
    "    #we only take into account the ratings of these 2 persons and we also drop all NaN\n",
    "    #from both of their ratings so we can only have movies they both watched    \n",
    "    df_1=prefs.loc[[person1, person2],:].dropna(axis='columns')\n",
    "    #we are measuring the correlation between the remaining ratings of person 1 and person 2\n",
    "    scores=df_1.loc[person1].corr(df_1.loc[person2])\n",
    "    #if we don't have any similar rating or if the standard deviation is 0, then we return a 0 instead of Nan\n",
    "    if np.isnan(scores)==True:\n",
    "            scores=0\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calling our function to test it with the 100K dataset\n",
    "# person_1='8'\n",
    "# person_2='4'\n",
    "# pearsonSimilarity(hundredKmovie,person_1,person_2)\n",
    "# # #result: 0.1178511301977579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36514837167011077"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calling our function to test it with the 20x20 dataset\n",
    "person1='4656: Victoria'\n",
    "person2='139: Nuria'\n",
    "pearsonSimilarity(movieratings,person1,person2)\n",
    "#result: 0.36514837167011077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function that returns recommendations for the movies that 'person' didn't watch!\n",
    "# in the case that no one watched the movie yet, it returns a 0 value for that movie!\n",
    "def getRecommendations(prefs,person,similarity=pearsonSimilarity):\n",
    "    #just in case the 'User' is not in index, I added it!\n",
    "    if 'User' in prefs.columns:\n",
    "        prefs['User']=prefs['User'].astype(str)\n",
    "        prefs=prefs.set_index('User')\n",
    "    #creating a dataframe that has all the movies that 'person' watched! Starting by taking into account only\n",
    "    #his ratings then by dropping NaN(movies he didn't watch) \n",
    "    titlesperson=pd.DataFrame(prefs.loc[person,:].dropna())\n",
    "    #finally droping the person column from titlesperson! so we get a 1 column dataset!\n",
    "    titlesperson=titlesperson.drop(columns=[person])\n",
    "    replica=prefs\n",
    "    #iterating over all movies that the person watched and dropping them from the dataframe\n",
    "    for title in titlesperson.index:\n",
    "            replica=replica.drop(columns=title)\n",
    "    #creating 2 lists to use in the for loop\n",
    "    list_1=[]\n",
    "    list_2=[]\n",
    "    #iterating over every user, in order to compute the similarity of person with each of the other users!\n",
    "    for i in replica.index:\n",
    "        #checking that we're not iterating over the same person that we're getting recomm for\n",
    "        if i != person:\n",
    "            #we then call the similarity function in order to get the similarities\n",
    "            list_1=(i,similarity(prefs,person,i))\n",
    "            list_2.append(list_1)\n",
    "            \n",
    "    #we create a dataframe out of recommendations and merge it with the initial dataset\n",
    "    list_2=pd.DataFrame(list_2)\n",
    "    list_2.columns=['User','Similarity']\n",
    "    list_2=list_2.dropna()\n",
    "    #replacing the negative similarities with 0! because we only want to make positive recom\n",
    "    list_2.loc[list_2['Similarity'] <0] = 0\n",
    "    #merging the 2 datasets\n",
    "    merged = pd.DataFrame.merge( list_2,replica, on='User',left_on=None, right_on=None,)\n",
    "    #filling the Nan with 0 because we will multiply later and they would be not taken into consideration\n",
    "    merged=merged.fillna(0)\n",
    "    list_3=[]\n",
    "    list_4=[]\n",
    "    #iterating over all the columns, in order to apply the formula to each of the movies!\n",
    "    #and get the recommendations!\n",
    "    for j in merged.columns:\n",
    "        #making sure that we don't iterate over the 'User' and the Similarity one!\n",
    "        if (j != 'User') and (j!='Similarity'):\n",
    "            #creating a new column that is equal to 1 if the rating is different than 0 and 0 otherwise\n",
    "            merged.loc[merged[j] !=0, 'SIM'] = 1\n",
    "            merged.loc[merged[j] ==0, 'SIM'] = 0\n",
    "            #checking that our denominator is not 0! if it is we return the value 0 for that movie!\n",
    "            if sum(merged['Similarity'].mul(merged['SIM'])) <= 0.01:\n",
    "                r=0\n",
    "            else:\n",
    "                #only taking into account the similarity of the users who rated that specific movie!\n",
    "                value_1=merged['Similarity'].mul(merged[j])\n",
    "                r=(sum(value_1)/sum(merged['Similarity'].mul(merged['SIM'])))\n",
    "                # set the upper threshold to 5 because in some cases it reached 5.0000001\n",
    "                if r > 5:\n",
    "                    r=5\n",
    "            list_3=(r,j)\n",
    "            list_4.append(list_3)\n",
    "    rankings = sorted(list_4, key=lambda tup: tup[0],reverse=True)\n",
    "\n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.804929227387678, '2571: Matrix, The (1999)'),\n",
       " (3.252404115805032, '318: Shawshank Redemption, The (1994)'),\n",
       " (3.001761291308658, '356: Forrest Gump (1994)'),\n",
       " (2.9987765717943913, '2396: Shakespeare in Love (1998)'),\n",
       " (2.775452598428217, '2762: Sixth Sense, The (1999)'),\n",
       " (2.1518724871008907, '541: Blade Runner (1982)'),\n",
       " (1.796481125969382, '2028: Saving Private Ryan (1998)')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calling our function to test it with the 20x20 dataset\n",
    "# it takes 0.16 seconds to run\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "person1='4656: Victoria'\n",
    "getRecommendations(movieratings,person1,similarity=pearsonSimilarity)\n",
    "# print (\"My program took\", time.time() - start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT JUST IN CASE YOU WERE NOT ABLE TO LOAD THEM\n",
    "# [(3.804929227387678, '2571: Matrix, The (1999)'),\n",
    "#  (3.252404115805032, '318: Shawshank Redemption, The (1994)'),\n",
    "#  (3.001761291308658, '356: Forrest Gump (1994)'),\n",
    "#  (2.9987765717943913, '2396: Shakespeare in Love (1998)'),\n",
    "#  (2.775452598428217, '2762: Sixth Sense, The (1999)'),\n",
    "#  (2.1518724871008907, '541: Blade Runner (1982)'),\n",
    "#  (1.796481125969382, '2028: Saving Private Ryan (1998)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # calling our function to test it with the 100K dataset\n",
    "# #takes 17.37 seconds to run!\n",
    "# # import time\n",
    "# # start_time = time.time()\n",
    "# person_1='8'\n",
    "# person_2='4'\n",
    "# getRecommendations(hundredKmovie,person_2,similarity=pearsonSimilarity)\n",
    "# # print (\"My program took\", time.time() - start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the n Users that have the highest similarity with 'person'\n",
    "# the similarities are from -1 to 1!\n",
    "# if no User have similarities with 'person', it will return zeroes\n",
    "def topMatches(prefs,person,n=5,similarity=pearsonSimilarity): \n",
    "    list_1=[]\n",
    "    list_2=[]\n",
    "    #just in case the 'User' is not in index, I added it!\n",
    "    if 'User' in prefs.columns:\n",
    "        prefs['User']=prefs['User'].astype(str)\n",
    "        prefs=prefs.set_index('User')\n",
    "    #iterating over all the users except the person we're checking the matches for!\n",
    "    for i in prefs.index:\n",
    "        if i != person:\n",
    "            #getting the similarity of 'person' with all the other users\n",
    "            list_1=(similarity(prefs,person,i),i)\n",
    "            #if the result is NaN (for some reason) we don't append in the list adn we move to the next user!\n",
    "            if np.isnan(list_1[0])==False:\n",
    "                list_2.append(list_1)\n",
    "    scores = sorted(list_2, key=lambda tup: tup[0],reverse=True)\n",
    "    return scores[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.944911182523068, '6037: Rachel'),\n",
       " (0.5976143046671967, '3823: Ana'),\n",
       " (0.578979445733232, '4790: Oriol'),\n",
       " (0.492592183071889, '5277: Maria'),\n",
       " (0.4273246726830629, '3118: Carles')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calling our function to test it with the 20x20 dataset\n",
    "person1='4656: Victoria'\n",
    "n=5\n",
    "topMatches(movieratings,person1,n=5,similarity=pearsonSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS JUST IN CASE YOU WERE NOT ABLE TO LOAD THEM\n",
    "# [(0.944911182523068, '6037: Rachel'),\n",
    "#  (0.5976143046671967, '3823: Ana'),\n",
    "#  (0.578979445733232, '4790: Oriol'),\n",
    "#  (0.492592183071889, '5277: Maria'),\n",
    "#  (0.4273246726830629, '3118: Carles')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calling our function to test it with the 100K dataset\n",
    "# person_1='8'\n",
    "# person_2='4'\n",
    "# topMatches(hundredKmovie,person_1,n=5,similarity=pearsonSimilarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
